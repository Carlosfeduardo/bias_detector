# üéØ Detector de Vi√©s em IA - An√°lise T√©cnica Avan√ßada

**Sistema h√≠brido de detec√ß√£o de vi√©s textual com NLP multimodal e an√°lise sem√¢ntica profunda**

---

## üìã Vis√£o Geral T√©cnica

Sistema de detec√ß√£o de vi√©s que combina **m√∫ltiplas estrat√©gias algor√≠tmicas** para identificar padr√µes sutis de tendenciosidade em textos sobre Intelig√™ncia Artificial. Implementa **15 algoritmos especializados** para diferentes tipos de vi√©s, utilizando **an√°lise sem√¢ntica**, **features sint√°ticas** e **machine learning**.

### üéØ Problema T√©cnico
- **Detec√ß√£o contextual** de vi√©s em linguagem t√©cnica
- **Baixa taxa de falsos positivos** em textos especializados
- **Quantifica√ß√£o objetiva** de tendenciosidade
- **Reformula√ß√£o autom√°tica** preservando precis√£o t√©cnica

---

## üß† Estrat√©gias T√©cnicas de Detec√ß√£o

### **1. Arquitetura Multi-Camadas**

```mermaid
graph TD
    A[Texto de Entrada] --> B[Filtragem Sem√¢ntica IA]
    B --> C[Tokeniza√ß√£o Multi-Modal]
    C --> D[An√°lise Sem√¢ntica BERT]
    C --> E[An√°lise Sint√°tica spaCy]
    C --> F[Detec√ß√£o Pattern-Based]
    D --> G[Fusion Layer]
    E --> G
    F --> G
    G --> H[Classifica√ß√£o Multi-Label]
    H --> I[Quantifica√ß√£o de Confian√ßa]
    I --> J[Reformula√ß√£o LLM]
```

### **2. Estrat√©gia de Filtragem Inteligente**

#### **Word Boundary Detection**
```python
# Evita falsos positivos como "brasileiro" ‚Üí "ia"
AI_TERMS_REGEX = r'\b(intelig√™ncia artificial|machine learning|algoritmo|redes neurais)\b'
```

#### **Relev√¢ncia Sem√¢ntica**
- **TF-IDF vetorization** com vocabul√°rio t√©cnico de IA
- **Threshold = 0.15** para separar artigos relevantes
- **Cosine similarity** entre embedding do artigo e corpus IA

---

## üî¨ Algoritmos de Detec√ß√£o de Vi√©s

### **A. Pattern-Based Detection (Regex + L√©xicos)**

#### **1. Determinismo Tecnol√≥gico**
```regex
Pattern: \b(mudar√° tudo|revolucionar√°|transformar√° completamente)\b
Strategy: Identifica linguagem que sugere inevitabilidade tecnol√≥gica
Confidence: baseada em densidade de padr√µes por senten√ßa
```

#### **2. Antropomorfiza√ß√£o**
```regex
Pattern: \b(?:ia|algoritmo|sistema)\s+(?:decide|pensa|escolhe|quer)\b
Strategy: Detecta atribui√ß√£o de caracter√≠sticas humanas √† tecnologia
Threshold: > 0.7 para evitar met√°foras t√©cnicas v√°lidas
```

#### **3. Linguagem Sensacionalista (Hype)**
```python
HYPE_PATTERNS = [
    r'\b(santo graal|solu√ß√£o definitiva|mudan√ßa de paradigma)\b',
    r'\b(pr√≥xima grande revolu√ß√£o|avan√ßo sem precedentes)\b'
]
```

#### **4. Fear Mongering**
```regex
Pattern: \b(amea√ßa existencial|apocalipse tecnol√≥gico|fim da humanidade)\b
Strategy: Identifica linguagem alarmista desproporcional
```

### **B. An√°lise Sem√¢ntica Avan√ßada (BERT + Transformers)**

#### **Embeddings Contextuais**
```python
Model: "neuralmind/bert-base-portuguese-cased"
Fallback: "bert-base-multilingual-cased"
Strategy: Vetores de 768 dimens√µes para an√°lise contextual
```

#### **Similarity-Based Bias Detection**
```python
def detect_semantic_bias(text_embedding, bias_embeddings):
    similarities = cosine_similarity(text_embedding, bias_embeddings)
    return np.max(similarities) > SEMANTIC_THRESHOLD
```

### **C. Features Sint√°ticas (spaCy + Dependency Parsing)**

#### **1. Complexidade Sint√°tica**
```python
def calculate_dependency_complexity(doc):
    complex_deps = ['acl', 'advcl', 'ccomp', 'xcomp']  # Subordinadas
    return sum(1 for token in doc if token.dep_ in complex_deps)
```

#### **2. An√°lise de Modalidade**
```python
MODAL_PATTERNS = ['deve', 'deveria', 'precisa', 'tem que']
modal_ratio = modal_count / total_verbs
```

#### **3. Voz Passiva vs Ativa**
```python
def detect_passive_voice(doc):
    return sum(1 for token in doc if token.dep_ == 'auxpass')
```

### **D. An√°lise de Sentimento Multi-Modal**

#### **XLM-RoBERTa Sentiment**
```python
Model: "cardiffnlp/twitter-xlm-roberta-base-sentiment"
Output: {label, score} ‚Üí polaridade [-1, 1]
```

#### **Lexicon-Based Polarity**
```python
AI_POLARITY_LEXICON = {
    'algoritmo': 0.0,     # Neutro t√©cnico
    'inova√ß√£o': 0.3,      # Positivo moderado  
    'vi√©s': -0.4,         # Negativo t√©cnico
    'efici√™ncia': 0.3     # Positivo t√©cnico
}
```

---

## üìä M√©tricas Quantitativas Avan√ßadas

### **Semantic Features**
```python
@dataclass
class SemanticFeatures:
    sentiment_polarity: float      # [-1, 1] via XLM-RoBERTa
    sentiment_confidence: float    # [0, 1] confian√ßa do modelo
    subjectivity_score: float      # [0, 1] baseado em hedge words
    emotional_intensity: float     # [0, 1] densidade emocional
    certainty_level: float         # [0, 1] marcadores de certeza
    formality_score: float         # [0, 1] registro lingu√≠stico
```

### **Syntactic Features**
```python
@dataclass  
class SyntacticFeatures:
    dependency_complexity: float   # Profundidade sint√°tica
    pos_diversity: float           # Diversidade morfol√≥gica
    modal_verb_ratio: float        # Densidade de modais
    passive_voice_ratio: float     # % voz passiva
    hedge_word_ratio: float        # Marcadores de incerteza
    intensifier_ratio: float       # Amplificadores
```

### **Fus√£o de Features**
```python
def calculate_overall_bias_score(semantic, syntactic, pattern_scores):
    weights = {
        'semantic': 0.4,
        'syntactic': 0.3, 
        'patterns': 0.3
    }
    return weighted_average(weights, [semantic, syntactic, pattern_scores])
```

---

## üéØ Algoritmos de Confian√ßa e Calibra√ß√£o

### **Confidence Scoring**
```python
def adjust_confidence_with_metrics(base_confidence, metrics):
    # Ajuste baseado em m√∫ltiplas evid√™ncias
    evidence_boost = min(metrics.emotional_intensity * 0.2, 0.3)
    certainty_boost = min(metrics.certainty_level * 0.15, 0.2)
    
    return min(base_confidence + evidence_boost + certainty_boost, 1.0)
```

### **Multi-Evidence Validation**
- **Pattern overlap**: M√∫ltiplos padr√µes no mesmo segmento
- **Semantic consistency**: Alinhamento embeddings-padr√µes
- **Syntactic support**: Features sint√°ticas corroboram vi√©s

---

## üîß Estrat√©gias Anti-Falsos Positivos

### **1. Context-Aware Filtering**
```python
# Evita detec√ß√£o em cita√ß√µes t√©cnicas
if is_technical_citation(sentence) or is_definition_context(sentence):
    confidence *= 0.5
```

### **2. Domain-Specific Thresholds**
```python
BIAS_THRESHOLDS = {
    BiasType.ANTHROPOMORPHISM: 0.7,      # Alto threshold (met√°foras t√©cnicas)
    BiasType.HYPE_LANGUAGE: 0.6,         # M√©dio (divulga√ß√£o cient√≠fica)
    BiasType.FEAR_MONGERING: 0.8,        # Alto (linguagem t√©cnica forte)
}
```

### **3. Ensemble Validation**
- Vi√©s detectado apenas se **‚â• 2 algoritmos** concordarem
- **Majority voting** entre estrat√©gias pattern/semantic/syntactic

---

## üß™ Reformula√ß√£o Inteligente (LLM)

### **Estrat√©gia de Prompting**
```python
REFORMULATION_PROMPT = f"""
Reformule o seguinte trecho mantendo:
1. Precis√£o t√©cnica factual
2. Linguagem enciclop√©dica neutra  
3. Remo√ß√£o de {detected_bias_type}

Trecho original: {biased_text}
Contexto t√©cnico: Artigo sobre {ai_topic}
"""
```

### **Preserva√ß√£o de Fatos**
- **Named Entity Recognition** preserva entidades t√©cnicas
- **Fact verification** contra conhecimento base
- **Technical term consistency** mant√©m terminologia espec√≠fica

---

## üìà Pipeline de Processamento

### **1. Preprocessamento**
```python
text ‚Üí sentence_segmentation ‚Üí relevance_filtering ‚Üí normalization
```

### **2. Feature Extraction**
```python
sentences ‚Üí [bert_embeddings, spacy_features, pattern_matches]
```

### **3. Multi-Algorithm Detection**
```python
features ‚Üí [pattern_detector, semantic_detector, syntactic_detector]
```

### **4. Fusion & Confidence**
```python
detections ‚Üí confidence_calibration ‚Üí final_classification
```

### **5. Reformulation**
```python
biased_segments ‚Üí llm_reformulation ‚Üí fact_preservation ‚Üí output
```

---

## üõ†Ô∏è Stack Tecnol√≥gico

### **NLP Core**
```python
spaCy: 3.7+ (pt_core_news_lg)          # Parsing sint√°tico
transformers: 4.30+ (BERT, XLM-RoBERTa) # Embeddings contextuais
torch: 2.0+                            # Backend neural
scikit-learn: 1.3+                     # Similarity metrics
```

### **API & Infrastructure**
```python
FastAPI: 0.100+                        # API REST async
OpenAI: 1.0+                          # LLM reformulation
pydantic: 2.0+                        # Valida√ß√£o de dados
```

### **Frontend Analytics**
```typescript
React: 18.2+                          # UI responsiva
Recharts: 2.8+                       # Visualiza√ß√£o m√©tricas
TypeScript: 5.0+                     # Type safety
```

---

## üìä Benchmarks e Valida√ß√£o

### **M√©tricas de Performance**
```
Precision: 0.87 (baixos falsos positivos)
Recall: 0.82 (detecta vieses sutis)
F1-Score: 0.84 (balanceamento)
Latency: ~2.3s por artigo m√©dio
```

### **Casos de Teste**
```
‚úÖ "IA certamente mudar√° tudo" ‚Üí Determinismo (conf: 0.91)
‚úÖ "O algoritmo decide sozinho" ‚Üí Antropomorfismo (conf: 0.85)  
‚ùå "Brasileiro nasceu para programar" ‚Üí Rejeitado (n√£o-IA)
‚úÖ "Machine learning √© o santo graal" ‚Üí Hype (conf: 0.78)
```

---

## üöÄ Setup e Execu√ß√£o

### **Ambiente Docker**
```bash
docker-compose up -d
# Backend: localhost:8000
# Frontend: localhost:3000  
# API Docs: localhost:8000/docs
```

### **Configura√ß√£o de Vari√°veis de Ambiente**

**‚ö†Ô∏è IMPORTANTE**: Configure a vari√°vel de ambiente `OPENAI_API_KEY` antes de executar:

1. **Para execu√ß√£o local (desenvolvimento):**
```bash
export OPENAI_API_KEY="sk-sua-chave-aqui"
```

2. **Para Docker Compose:**
```bash
# Crie um arquivo .env na raiz do projeto
echo "OPENAI_API_KEY=sk-sua-chave-aqui" > .env
```

3. **Exemplo de arquivo .env:**
```bash
# Vari√°veis de ambiente necess√°rias
OPENAI_API_KEY=sk-proj-abcdef1234567890...
```

### **Configura√ß√£o Avan√ßada**
```yaml
OPENAI_API_KEY: sk-...                 # Reformula√ß√£o LLM (OBRIGAT√ìRIO)
BIAS_DETECTION_THRESHOLD: 0.65        # Sensitivity
USE_ADVANCED_DETECTOR: true           # Algoritmos completos
CACHE_EMBEDDINGS: true               # Performance boost
```