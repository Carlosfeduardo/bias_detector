from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn
import os
from typing import List, Optional, Dict, Any
import time
import asyncio

from .models import AnalyzeRequest, AnalyzeResponse, ErrorResponse, BiasAnalysis, AnalysisRequest, AnalysisResponse
from .wikipedia_client import WikipediaClient
from .bias_detector import BiasDetector
from .reformulator import TextReformulator
from .utils import validate_article_title, normalize_text, truncate_text

# Import condicional do detector avan√ßado
try:
    from .advanced_bias_detector import AdvancedBiasDetector
    ADVANCED_DETECTOR_AVAILABLE = True
    print("‚úÖ Detector avan√ßado dispon√≠vel")
except ImportError as e:
    ADVANCED_DETECTOR_AVAILABLE = False
    print(f"‚ö†Ô∏è Detector avan√ßado n√£o dispon√≠vel: {e}")
    print("üí° Usando apenas detector b√°sico")

# Configura√ß√£o da API OpenAI a partir de vari√°vel de ambiente
API_KEY_OPENAI = os.getenv("OPENAI_API_KEY")
if not API_KEY_OPENAI:
    raise ValueError("Vari√°vel de ambiente OPENAI_API_KEY n√£o encontrada. Configure sua chave da API OpenAI.")
print("‚úÖ Chave OpenAI carregada da vari√°vel de ambiente")

# Inicializa√ß√£o da aplica√ß√£o
app = FastAPI(
    title="Detector de Vi√©s em Artigos da Wikipedia",
    description="API para detectar e reformular vi√©s textual em artigos da Wikipedia sobre IA",
    version="1.0.0"
)

# Configura√ß√£o do CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://localhost:3001", 
        "https://biasdetector.online",
        "https://www.biasdetector.online",
        "http://biasdetector.online",
        "http://www.biasdetector.online",
        "http://103.199.184.185",
        "https://103.199.184.185"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Inicializa√ß√£o dos componentes
wikipedia_client = WikipediaClient()
bias_detector = BiasDetector()
text_reformulator = TextReformulator(API_KEY_OPENAI)

# Inicializa√ß√£o condicional do detector avan√ßado
if ADVANCED_DETECTOR_AVAILABLE:
    try:
        advanced_bias_detector = AdvancedBiasDetector()
        print("‚úÖ Detector avan√ßado inicializado")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao inicializar detector avan√ßado: {e}")
        advanced_bias_detector = None
        ADVANCED_DETECTOR_AVAILABLE = False
else:
    advanced_bias_detector = None

# New models for detailed progress tracking
class AnalysisStep(BaseModel):
    id: str
    title: str
    description: str
    status: str  # 'waiting', 'running', 'completed', 'error'
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    details: Optional[List[str]] = None
    metrics: Optional[Dict[str, Any]] = None

class DetailedAnalysisResponse(BaseModel):
    success: bool
    steps: List[AnalysisStep]
    final_result: Optional[AnalysisResponse] = None
    total_duration: Optional[float] = None
    error_message: Optional[str] = None

@app.get("/")
async def root():
    """Endpoint raiz com informa√ß√µes da API"""
    return {
        "message": "Detector de Vi√©s em Artigos da Wikipedia",
        "version": "1.0.0",
        "status": "ativo",
        "endpoints": {
            "analyze": "/analyze - Analisa vi√©s em artigo da Wikipedia",
            "health": "/health - Verifica sa√∫de da API"
        }
    }

@app.get("/health")
async def health_check():
    """Endpoint para verificar sa√∫de da API"""
    return {
        "status": "healthy",
        "components": {
            "wikipedia_client": "ok",
            "bias_detector": "ok",
            "text_reformulator": "ok"
        }
    }

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_article(request: AnalyzeRequest):
    """
    Analisa um artigo da Wikipedia em busca de vi√©s textual
    
    Args:
        request: Objeto contendo o t√≠tulo do artigo
        
    Returns:
        AnalyzeResponse: An√°lise completa do artigo com vi√©s detectado
        
    Raises:
        HTTPException: Em caso de erro no processamento
    """
    try:
        # Valida√ß√£o do t√≠tulo
        if not validate_article_title(request.titulo_artigo):
            raise HTTPException(
                status_code=400,
                detail="T√≠tulo do artigo inv√°lido. Use apenas caracteres alfanum√©ricos e espa√ßos."
            )
        
        # Busca o artigo na Wikipedia
        print(f"Buscando artigo: {request.titulo_artigo}")
        article_data = wikipedia_client.get_article_content(request.titulo_artigo)
        
        if not article_data:
            raise HTTPException(
                status_code=404,
                detail=f"Artigo '{request.titulo_artigo}' n√£o encontrado na Wikipedia portuguesa."
            )
        
        # Verifica se o artigo √© relacionado √† IA
        if not wikipedia_client.is_ai_related(article_data['title'], article_data['content']):
            raise HTTPException(
                status_code=400,
                detail="Este artigo n√£o parece ser relacionado √† Intelig√™ncia Artificial ou √°reas correlatas."
            )
        
        # Normaliza o conte√∫do
        normalized_content = normalize_text(article_data['content'])
        
        if len(normalized_content) < 100:
            raise HTTPException(
                status_code=400,
                detail="Artigo muito curto para an√°lise de vi√©s."
            )
        
        # Calcula total de segmentos analisados (senten√ßas)
        total_segments_analyzed = 0
        if ADVANCED_DETECTOR_AVAILABLE and advanced_bias_detector and advanced_bias_detector.nlp:
            doc = advanced_bias_detector.nlp(normalized_content)
            total_segments_analyzed = len([sent for sent in doc.sents if len(sent.text.strip()) >= 20])
        else:
            # Fallback: estima baseado em pontua√ß√£o
            total_segments_analyzed = len([s.strip() for s in normalized_content.split('.') if len(s.strip()) >= 20])
        
        # Escolhe o detector baseado na prefer√™ncia e disponibilidade
        if request.usar_detector_avancado and ADVANCED_DETECTOR_AVAILABLE and advanced_bias_detector:
            print("üß† Usando detector avan√ßado...")
            try:
                advanced_analyses = advanced_bias_detector.analyze_text_advanced(normalized_content)
                
                # Converte an√°lises avan√ßadas para formato b√°sico
                bias_analyses = []
                for adv_analysis in advanced_analyses:
                    if adv_analysis.confidence_scores:
                        # Seleciona o tipo de vi√©s com maior confian√ßa
                        best_bias_type = max(adv_analysis.confidence_scores.items(), key=lambda x: x[1])
                        bias_type, confidence = best_bias_type
                        
                        basic_analysis = BiasAnalysis(
                            trecho_original=adv_analysis.text_segment,
                            tipo_vies=bias_type,
                            explicacao=adv_analysis.explanation,
                            reformulacao_sugerida="",
                            posicao_inicio=adv_analysis.start_pos,
                            posicao_fim=adv_analysis.end_pos,
                            confianca=confidence,
                            intensidade_emocional=adv_analysis.semantic_features.emotional_intensity,
                            polaridade_sentimento=adv_analysis.semantic_features.sentiment_polarity,
                            complexidade_sintatica=adv_analysis.syntactic_features.dependency_complexity,
                            nivel_certeza=adv_analysis.semantic_features.certainty_level,
                            score_formalidade=adv_analysis.semantic_features.formality_score
                        )
                        bias_analyses.append(basic_analysis)
                        
            except Exception as e:
                print(f"Erro no detector avan√ßado, usando b√°sico: {e}")
                bias_analyses = bias_detector.analyze_text(normalized_content)
        else:
            print("üìù Usando detector b√°sico melhorado...")
            bias_analyses = bias_detector.analyze_text(normalized_content)
        
        # Calcula m√©tricas agregadas
        metricas_gerais = {}
        if bias_analyses:
            metricas_gerais = {
                'polaridade_media': sum(a.polaridade_sentimento or 0 for a in bias_analyses) / len(bias_analyses),
                'intensidade_emocional_media': sum(a.intensidade_emocional or 0 for a in bias_analyses) / len(bias_analyses),
                'complexidade_media': sum(a.complexidade_sintatica or 0 for a in bias_analyses) / len(bias_analyses),
                'nivel_certeza_medio': sum(a.nivel_certeza or 0 for a in bias_analyses) / len(bias_analyses),
                'score_formalidade_medio': sum(a.score_formalidade or 0 for a in bias_analyses) / len(bias_analyses)
            }
        
        # Distribui√ß√£o dos tipos de vi√©s
        distribuicao_tipos = {}
        for analysis in bias_analyses:
            tipo = analysis.tipo_vies.value
            distribuicao_tipos[tipo] = distribuicao_tipos.get(tipo, 0) + 1
        
        if not bias_analyses:
            # Retorna resposta mesmo sem vi√©s detectado
            return AnalyzeResponse(
                titulo=article_data['title'],
                conteudo_original=normalized_content,
                url_wikipedia=article_data['url'],
                analises_vies=[],
                resumo_geral=f"Nenhum vi√©s significativo foi detectado no artigo '{article_data['title']}'. O artigo foi analisado em {total_segments_analyzed} segmentos e nenhum apresentou vi√©s detect√°vel.",
                total_trechos_analisados=total_segments_analyzed,
                total_trechos_com_vies=0,
                score_polaridade_geral=0.0,
                score_emocional_geral=0.0,
                score_complexidade_geral=0.0,
                distribuicao_tipos_vies={}
            )
        
        # Reformula os trechos com vi√©s
        print("Reformulando trechos com vi√©s...")
        reformulated_analyses = text_reformulator.reformulate_analyses(bias_analyses)
        
        # Gera resumo geral expandido
        print("Gerando resumo geral expandido...")
        resumo_base = text_reformulator.generate_general_summary(
            reformulated_analyses, 
            article_data['title']
        )
        
        # Adiciona estat√≠sticas ao resumo
        resumo_expandido = f"""{resumo_base}

üìä **M√©tricas Quantitativas:**
- Polaridade m√©dia do sentimento: {metricas_gerais.get('polaridade_media', 0):.2f} (-1 a 1)
- Intensidade emocional m√©dia: {metricas_gerais.get('intensidade_emocional_media', 0):.2f} (0 a 1)
- Complexidade sint√°tica m√©dia: {metricas_gerais.get('complexidade_media', 0):.2f} (0 a 1)
- N√≠vel de certeza m√©dio: {metricas_gerais.get('nivel_certeza_medio', 0):.2f} (0 a 1)
- Score de formalidade m√©dio: {metricas_gerais.get('score_formalidade_medio', 0):.2f} (0 a 1)

üéØ **Distribui√ß√£o dos Tipos de Vi√©s:**
""" + "\n".join([f"- {tipo.replace('_', ' ').title()}: {count} ocorr√™ncia(s)" for tipo, count in distribuicao_tipos.items()])
        
        # Monta resposta
        total_com_vies = len(reformulated_analyses)
        print(f"DEBUG: total_segments_analyzed={total_segments_analyzed}, total_com_vies={total_com_vies}")
        
        response = AnalyzeResponse(
            titulo=article_data['title'],
            conteudo_original=normalized_content,
            url_wikipedia=article_data['url'],
            analises_vies=reformulated_analyses,
            resumo_geral=resumo_expandido,
            total_trechos_analisados=total_segments_analyzed,
            total_trechos_com_vies=total_com_vies,
            score_polaridade_geral=metricas_gerais.get('polaridade_media', 0.0),
            score_emocional_geral=metricas_gerais.get('intensidade_emocional_media', 0.0),
            score_complexidade_geral=metricas_gerais.get('complexidade_media', 0.0),
            distribuicao_tipos_vies=distribuicao_tipos
        )
        
        print(f"DEBUG: Modelo criado com campos: {list(response.model_dump().keys())}")
        
        detector_usado = "avan√ßado" if (request.usar_detector_avancado and ADVANCED_DETECTOR_AVAILABLE) else "b√°sico melhorado"
        print(f"An√°lise conclu√≠da ({detector_usado}): {len(reformulated_analyses)} trechos com vi√©s detectados")
        return response
        
    except HTTPException:
        # Re-lan√ßa HTTPExceptions
        raise
    except Exception as e:
        print(f"Erro interno: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro interno do servidor: {str(e)}"
        )

@app.get("/test-wikipedia/{title}")
async def test_wikipedia_search(title: str):
    """Endpoint de teste para buscar artigos na Wikipedia"""
    try:
        article_data = wikipedia_client.get_article_content(title)
        
        if not article_data:
            return {"error": "Artigo n√£o encontrado"}
        
        # Retorna vers√£o truncada para teste
        return {
            "title": article_data['title'],
            "content_preview": truncate_text(article_data['content'], 500),
            "url": article_data['url'],
            "is_ai_related": wikipedia_client.is_ai_related(
                article_data['title'], 
                article_data['content']
            ),
            "content_length": len(article_data['content'])
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.get("/test-bias-detection")
async def test_bias_detection():
    """Endpoint de teste para detec√ß√£o de vi√©s"""
    test_text = """
    A intelig√™ncia artificial √© obviamente a tecnologia mais revolucion√°ria da nossa era. 
    Todos os especialistas concordam que ela mudar√° completamente nossa sociedade. 
    √â imposs√≠vel imaginar um futuro sem IA, pois ela √© claramente superior aos m√©todos tradicionais.
    Nunca houve uma tecnologia t√£o fant√°stica e transformadora.
    """
    
    try:
        analyses = bias_detector.analyze_text(test_text)
        return {
            "test_text": test_text,
            "bias_analyses": [
                {
                    "type": analysis.tipo_vies,
                    "confidence": analysis.confianca,
                    "explanation": analysis.explicacao,
                    "original_text": analysis.trecho_original
                }
                for analysis in analyses
            ],
            "total_detected": len(analyses)
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/analyze-advanced", response_model=dict)
async def analyze_article_advanced(request: AnalyzeRequest):
    """
    Analisa um artigo da Wikipedia usando t√©cnicas avan√ßadas de NLP
    
    Args:
        request: Objeto contendo o t√≠tulo do artigo
        
    Returns:
        Dict: An√°lise avan√ßada completa com m√©tricas detalhadas
        
    Raises:
        HTTPException: Em caso de erro no processamento
    """
    # Verifica se o detector avan√ßado est√° dispon√≠vel
    if not ADVANCED_DETECTOR_AVAILABLE or advanced_bias_detector is None:
        raise HTTPException(
            status_code=503,
            detail="Detector avan√ßado n√£o dispon√≠vel. Depend√™ncias de NLP n√£o instaladas. Use o endpoint /analyze para an√°lise b√°sica."
        )
    
    try:
        # Valida√ß√£o do t√≠tulo
        if not validate_article_title(request.titulo_artigo):
            raise HTTPException(
                status_code=400,
                detail="T√≠tulo do artigo inv√°lido. Use apenas caracteres alfanum√©ricos e espa√ßos."
            )
        
        # Busca o artigo na Wikipedia
        print(f"üîç Buscando artigo (an√°lise avan√ßada): {request.titulo_artigo}")
        article_data = wikipedia_client.get_article_content(request.titulo_artigo)
        
        if not article_data:
            raise HTTPException(
                status_code=404,
                detail=f"Artigo '{request.titulo_artigo}' n√£o encontrado na Wikipedia portuguesa."
            )
        
        # Verifica se o artigo √© relacionado √† IA
        if not wikipedia_client.is_ai_related(article_data['title'], article_data['content']):
            raise HTTPException(
                status_code=400,
                detail="Este artigo n√£o parece ser relacionado √† Intelig√™ncia Artificial ou √°reas correlatas."
            )
        
        # Normaliza o conte√∫do
        normalized_content = normalize_text(article_data['content'])
        
        if len(normalized_content) < 100:
            raise HTTPException(
                status_code=400,
                detail="Artigo muito curto para an√°lise de vi√©s."
            )
        
        # An√°lise avan√ßada de vi√©s
        print("üß† Executando an√°lise avan√ßada de vi√©s...")
        advanced_analyses = advanced_bias_detector.analyze_text_advanced(normalized_content)
        
        if not advanced_analyses:
            return {
                "status": "no_bias_detected",
                "title": article_data['title'],
                "url": article_data['url'],
                "content_length": len(normalized_content),
                "message": "Nenhum vi√©s significativo detectado na an√°lise avan√ßada",
                "analysis_type": "advanced_nlp"
            }
        
        # Gera relat√≥rio abrangente
        print("üìä Gerando relat√≥rio abrangente...")
        comprehensive_report = advanced_bias_detector.generate_comprehensive_report(advanced_analyses)
        
        # Converte analyses para formato compat√≠vel
        converted_analyses = []
        for analysis in advanced_analyses:
            converted_analysis = {
                "text_segment": analysis.text_segment,
                "start_pos": analysis.start_pos,
                "end_pos": analysis.end_pos,
                "bias_types": [bt.value for bt in analysis.bias_types],
                "confidence_scores": {bt.value: score for bt, score in analysis.confidence_scores.items()},
                "overall_bias_score": analysis.overall_bias_score,
                "explanation": analysis.explanation,
                "evidence": analysis.evidence,
                "reformulation_suggestions": analysis.reformulation_suggestions,
                "semantic_features": {
                    "sentiment_polarity": analysis.semantic_features.sentiment_polarity,
                    "sentiment_confidence": analysis.semantic_features.sentiment_confidence,
                    "subjectivity_score": analysis.semantic_features.subjectivity_score,
                    "emotional_intensity": analysis.semantic_features.emotional_intensity,
                    "certainty_level": analysis.semantic_features.certainty_level,
                    "formality_score": analysis.semantic_features.formality_score
                },
                "syntactic_features": {
                    "dependency_complexity": analysis.syntactic_features.dependency_complexity,
                    "pos_diversity": analysis.syntactic_features.pos_diversity,
                    "modal_verb_ratio": analysis.syntactic_features.modal_verb_ratio,
                    "passive_voice_ratio": analysis.syntactic_features.passive_voice_ratio,
                    "hedge_word_ratio": analysis.syntactic_features.hedge_word_ratio,
                    "intensifier_ratio": analysis.syntactic_features.intensifier_ratio
                }
            }
            converted_analyses.append(converted_analysis)
        
        # Reformula trechos usando an√°lise avan√ßada
        print("‚úèÔ∏è Reformulando trechos com IA...")
        advanced_reformulations = []
        for analysis in advanced_analyses[:5]:  # Limita a 5 para n√£o sobrecarregar a API
            try:
                # Cria BiasAnalysis tempor√°rio para o reformulador
                temp_analysis = BiasAnalysis(
                    trecho_original=analysis.text_segment,
                    tipo_vies=analysis.bias_types[0] if analysis.bias_types else BiasType.LOADED_LANGUAGE,
                    explicacao=analysis.explanation,
                    reformulacao_sugerida="",
                    posicao_inicio=analysis.start_pos,
                    posicao_fim=analysis.end_pos,
                    confianca=analysis.overall_bias_score
                )
                
                reformulated = text_reformulator.reformulate_analyses([temp_analysis])
                if reformulated:
                    advanced_reformulations.append({
                        "original": analysis.text_segment,
                        "reformulated": reformulated[0].reformulacao_sugerida,
                        "confidence": analysis.overall_bias_score,
                        "bias_types": [bt.value for bt in analysis.bias_types]
                    })
            except Exception as e:
                print(f"Erro na reformula√ß√£o: {e}")
        
        # Monta resposta avan√ßada
        response = {
            "status": "advanced_analysis_completed",
            "analysis_type": "advanced_nlp",
            "title": article_data['title'],
            "url": article_data['url'],
            "content_length": len(normalized_content),
            "total_biased_segments": len(advanced_analyses),
            "advanced_analyses": converted_analyses,
            "comprehensive_report": comprehensive_report,
            "reformulations": advanced_reformulations,
            "analysis_metadata": {
                "models_used": {
                    "spacy": "pt_core_news_sm/lg",
                    "bert": "neuralmind/bert-base-portuguese-cased or multilingual",
                    "sentiment": "cardiffnlp/twitter-xlm-roberta-base-sentiment",
                    "reformulation": "openai/gpt-4o-mini"
                },
                "features_analyzed": [
                    "semantic_features", "syntactic_features", "semantic_frames",
                    "bert_embeddings", "dependency_parsing", "pos_tagging"
                ],
                "bias_detection_methods": [
                    "pattern_matching", "feature_engineering", "semantic_similarity",
                    "sentiment_analysis", "linguistic_markers"
                ]
            }
        }
        
        print(f"‚úÖ An√°lise avan√ßada conclu√≠da: {len(advanced_analyses)} segmentos analisados")
        return response
        
    except HTTPException:
        # Re-lan√ßa HTTPExceptions
        raise
    except Exception as e:
        print(f"Erro interno na an√°lise avan√ßada: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro interno do servidor na an√°lise avan√ßada: {str(e)}"
        )

@app.get("/test-metrics")
async def test_metrics():
    """Endpoint de teste para verificar se m√©tricas aparecem"""
    test_response = AnalysisResponse(
        article_title="Teste",
        article_url="http://test.com",
        article_content="Conte√∫do de teste",
        ai_related=True,
        bias_detected=False,
        overall_bias_score=0.0,
        bias_categories=[],
        detailed_analysis="Teste",
        metricas_quantitativas={"teste_direto": 999.999}
    )
    return test_response

@app.get("/models-status")
async def check_models_status():
    """Verifica status dos modelos de NLP carregados"""
    status = {
        "basic_detector": {
            "loaded": True,
            "description": "Detector b√°sico baseado em regex"
        },
        "advanced_detector": {
            "loaded": ADVANCED_DETECTOR_AVAILABLE and advanced_bias_detector is not None,
            "available": ADVANCED_DETECTOR_AVAILABLE
        },
        "reformulator": {
            "openai_integration": "dispon√≠vel",
                            "model": "gpt-4o-mini"
        }
    }
    
    # Adiciona detalhes do detector avan√ßado se dispon√≠vel
    if ADVANCED_DETECTOR_AVAILABLE and advanced_bias_detector is not None:
        try:
            status["advanced_detector"].update({
                "spacy_model": "dispon√≠vel" if advanced_bias_detector.nlp else "n√£o dispon√≠vel",
                "bert_model": "dispon√≠vel" if advanced_bias_detector.bert_model else "n√£o dispon√≠vel", 
                "sentiment_analyzer": "dispon√≠vel" if advanced_bias_detector.sentiment_analyzer else "n√£o dispon√≠vel"
            })
        except Exception as e:
            status["advanced_detector"]["error"] = str(e)
    else:
        status["advanced_detector"]["message"] = "Depend√™ncias avan√ßadas n√£o instaladas"
    
    return status

@app.post("/analyze-detailed", response_model=DetailedAnalysisResponse)
async def analyze_article_detailed(request: AnalysisRequest):
    """
    Analyze Wikipedia article with detailed step-by-step progress tracking
    """
    start_total_time = time.time()
    
    # Initialize all steps
    steps = [
        AnalysisStep(
            id="validation",
            title="1. Valida√ß√£o de Entrada",
            description="Verificando t√≠tulo do artigo e par√¢metros",
            status="waiting"
        ),
        AnalysisStep(
            id="wikipedia-search",
            title="2. Busca na Wikipedia",
            description="Procurando artigo na Wikipedia portuguesa",
            status="waiting"
        ),
        AnalysisStep(
            id="content-extraction",
            title="3. Extra√ß√£o de Conte√∫do",
            description="Baixando e limpando conte√∫do do artigo",
            status="waiting"
        ),
        AnalysisStep(
            id="ai-relevance",
            title="4. Verifica√ß√£o de Relev√¢ncia IA",
            description="Verificando se artigo √© relacionado √† IA",
            status="waiting"
        ),
        AnalysisStep(
            id="bias-detection",
            title="5. Detec√ß√£o de Vi√©s",
            description="Analisando texto em busca de vi√©s",
            status="waiting"
        ),
        AnalysisStep(
            id="reformulation",
            title="6. Reformula√ß√£o com IA",
            description="Gerando sugest√µes de reformula√ß√£o",
            status="waiting"
        ),
        AnalysisStep(
            id="summary-generation",
            title="7. Gera√ß√£o de Resumo",
            description="Criando resumo final da an√°lise",
            status="waiting"
        )
    ]
    
    try:
        # Step 1: Validation
        current_step = next(s for s in steps if s.id == "validation")
        current_step.status = "running"
        current_step.start_time = time.time()
        current_step.details = [
            f"Validando t√≠tulo: '{request.title}'",
            f"Modo avan√ßado: {'Dispon√≠vel' if ADVANCED_DETECTOR_AVAILABLE else 'N√£o dispon√≠vel'}",
            "Verificando par√¢metros de entrada..."
        ]
        
        if not request.title or len(request.title.strip()) < 2:
            current_step.status = "error"
            current_step.end_time = time.time()
            return DetailedAnalysisResponse(
                success=False,
                steps=steps,
                error_message="T√≠tulo do artigo √© obrigat√≥rio e deve ter pelo menos 2 caracteres"
            )
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "t√≠tulo_v√°lido": True,
            "caracteres": len(request.title),
            "modo_avan√ßado": ADVANCED_DETECTOR_AVAILABLE
        }
        
        # Step 2: Wikipedia Search
        current_step = next(s for s in steps if s.id == "wikipedia-search")
        current_step.status = "running"
        current_step.start_time = time.time()
        current_step.details = [
            "Conectando √† API da Wikipedia...",
            f"Buscando por: {request.title}",
            "Verificando disponibilidade do artigo..."
        ]
        
        # Simulate some processing time for visual effect
        await asyncio.sleep(0.5)
        
        wikipedia_result = wikipedia_client.get_article_content(request.title)
        
        if not wikipedia_result:
            current_step.status = "error"
            current_step.end_time = time.time()
            return DetailedAnalysisResponse(
                success=False,
                steps=steps,
                error_message=f"Artigo '{request.title}' n√£o encontrado na Wikipedia portuguesa."
            )
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "artigo_encontrado": True,
            "url": wikipedia_result["url"],
            "tamanho_artigo": len(wikipedia_result["content"]) if wikipedia_result["content"] else 0
        }
        
        # Step 3: Content Extraction
        current_step = next(s for s in steps if s.id == "content-extraction")
        current_step.status = "running"
        current_step.start_time = time.time()
        current_step.details = [
            "Extraindo texto principal do artigo...",
            "Removendo formata√ß√£o wiki...",
            "Limpando conte√∫do HTML...",
            "Processando par√°grafos..."
        ]
        
        await asyncio.sleep(0.3)
        
        content = wikipedia_result["content"]
        word_count = len(content.split()) if content else 0
        char_count = len(content) if content else 0
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "palavras": word_count,
            "caracteres": char_count,
            "par√°grafos": content.count('\n\n') + 1 if content else 0
        }
        
        # Step 4: AI Relevance Check
        current_step = next(s for s in steps if s.id == "ai-relevance")
        current_step.status = "running"
        current_step.start_time = time.time()
        current_step.details = [
            "Analisando palavras-chave relacionadas √† IA...",
            "Verificando contexto sem√¢ntico...",
            "Calculando score de relev√¢ncia..."
        ]
        
        await asyncio.sleep(0.4)
        
        ai_relevance = wikipedia_client.is_ai_related(wikipedia_result["title"], content)
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "√©_relacionado_ia": ai_relevance,
            "algoritmo": "Detec√ß√£o por palavras-chave",
            "confian√ßa": 0.85 if ai_relevance else 0.15
        }
        
        # Step 5: Bias Detection
        current_step = next(s for s in steps if s.id == "bias-detection")
        current_step.status = "running"
        current_step.start_time = time.time()
        
        if ADVANCED_DETECTOR_AVAILABLE:
            current_step.details = [
                "Carregando modelos avan√ßados (spaCy, BERT)...",
                "An√°lise sem√¢ntica profunda...",
                "Detec√ß√£o de padr√µes lingu√≠sticos...",
                "Calculando m√©tricas quantitativas...",
                "An√°lise de sentimento contextual..."
            ]
            await asyncio.sleep(1.0)  # Advanced analysis takes longer
            
            advanced_analyses = advanced_bias_detector.analyze_text_advanced(content)
            analysis_method = "Avan√ßado (spaCy + BERT + XLM-RoBERTa)"
            
            # Converter AdvancedBiasAnalysis para BiasAnalysis b√°sico para compatibilidade
            bias_analyses = []
            for adv_analysis in advanced_analyses:
                # Seleciona o tipo de vi√©s com maior confian√ßa
                if adv_analysis.confidence_scores:
                    best_bias_type = max(adv_analysis.confidence_scores.items(), key=lambda x: x[1])
                    bias_type, confidence = best_bias_type
                    
                    basic_analysis = BiasAnalysis(
                        trecho_original=adv_analysis.text_segment,
                        tipo_vies=bias_type,
                        explicacao=adv_analysis.explanation,
                        reformulacao_sugerida="",  # Ser√° preenchido depois
                        posicao_inicio=adv_analysis.start_pos,
                        posicao_fim=adv_analysis.end_pos,
                        confianca=confidence
                    )
                    bias_analyses.append(basic_analysis)
        else:
            current_step.details = [
                "Analisando padr√µes de linguagem tendenciosa...",
                "Detectando termos subjetivos...",
                "Verificando linguagem emocional...",
                "Buscando opini√µes apresentadas como fatos..."
            ]
            await asyncio.sleep(0.6)
            
            bias_analyses = bias_detector.analyze_text(content)
            analysis_method = "B√°sico (Regex + NLP)"
        
        # Criar um objeto de resultado compat√≠vel com o que o resto do c√≥digo espera  
        bias_detected = len(bias_analyses) > 0  # Qualquer detec√ß√£o √© considerada vi√©s
        overall_bias_score = sum(analysis.confianca for analysis in bias_analyses) / len(bias_analyses) if bias_analyses else 0.0
        bias_categories = list(set(analysis.tipo_vies.value for analysis in bias_analyses))
        detailed_analysis = f"Encontrados {len(bias_analyses)} trechos com vi√©s em {len(bias_categories)} categorias diferentes."
        
        # Criar objeto de resultado compat√≠vel
        class AnalysisResult:
            def __init__(self, bias_detected, overall_bias_score, bias_categories, detailed_analysis):
                self.bias_detected = bias_detected
                self.overall_bias_score = overall_bias_score
                self.bias_categories = bias_categories
                self.detailed_analysis = detailed_analysis
        
        analysis_result = AnalysisResult(bias_detected, overall_bias_score, bias_categories, detailed_analysis)
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "m√©todo": analysis_method,
            "vi√©s_detectado": analysis_result.bias_detected,
            "score_geral": analysis_result.overall_bias_score,
            "categorias": len(analysis_result.bias_categories)
        }
        
        # Step 6: Reformulation
        current_step = next(s for s in steps if s.id == "reformulation")
        current_step.status = "running"
        current_step.start_time = time.time()
        current_step.details = [
            "Identificando trechos problem√°ticos...",
            "Gerando sugest√µes com OpenAI GPT...",
            "Criando vers√µes neutras...",
            "Validando melhorias..."
        ]
        
        await asyncio.sleep(0.8)
        
        reformulated_text = ""
        if ADVANCED_DETECTOR_AVAILABLE and analysis_result.bias_detected:
            try:
                # Reformula usando o m√©todo correto da classe TextReformulator
                reformulated_analyses = text_reformulator.reformulate_analyses(bias_analyses)
                reformulated_text = "\n\n".join([f"Trecho original: {analysis.trecho_original}\nVers√£o reformulada: {analysis.reformulacao_sugerida}" for analysis in reformulated_analyses[:3]])  # Limita a 3 exemplos
            except Exception as e:
                print(f"Reformulation error: {e}")
                reformulated_text = "Erro na reformula√ß√£o: " + str(e)
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "reformula√ß√£o_gerada": bool(reformulated_text),
            "caracteres_reformulados": len(reformulated_text),
                            "servi√ßo": "OpenAI GPT-4o-mini"
        }
        
        # Step 7: Summary Generation
        current_step = next(s for s in steps if s.id == "summary-generation")
        current_step.status = "running"
        current_step.start_time = time.time()
        current_step.details = [
            "Compilando resultados finais...",
            "Gerando resumo executivo...",
            "Calculando estat√≠sticas...",
            "Preparando recomenda√ß√µes..."
        ]
        
        await asyncio.sleep(0.3)
        
        # Calculate quantitative metrics for any text (even without bias)
        metricas_quantitativas = {}
        
        if ADVANCED_DETECTOR_AVAILABLE and advanced_bias_detector is not None:
            # Calculate basic semantic and syntactic features for the entire text
            try:
                semantic_features = advanced_bias_detector.analyze_semantic_features(content[:1000])  # Use first 1000 chars
                syntactic_features = advanced_bias_detector.analyze_syntactic_features(content[:1000])
                
                metricas_quantitativas = {
                    "polaridade_media": semantic_features.sentiment_polarity,
                    "intensidade_emocional_media": semantic_features.emotional_intensity,
                    "complexidade_media": syntactic_features.dependency_complexity,
                    "nivel_certeza_medio": semantic_features.certainty_level,
                    "score_formalidade_medio": semantic_features.formality_score,
                    "subjetividade_media": semantic_features.subjectivity_score
                }
                
                # Always try to aggregate metrics from any analyses
                if advanced_analyses:
                    comprehensive_report = advanced_bias_detector.generate_comprehensive_report(advanced_analyses)
                    if 'semantic_profile' in comprehensive_report:
                        semantic_profile = comprehensive_report['semantic_profile']
                        metricas_quantitativas.update({
                            "polaridade_media": semantic_profile.get('avg_sentiment_polarity', semantic_features.sentiment_polarity),
                            "intensidade_emocional_media": semantic_profile.get('avg_emotional_intensity', semantic_features.emotional_intensity),
                            "nivel_certeza_medio": semantic_profile.get('avg_certainty_level', semantic_features.certainty_level),
                            "score_formalidade_medio": semantic_profile.get('avg_formality', semantic_features.formality_score),
                            "subjetividade_media": semantic_profile.get('avg_subjectivity', semantic_features.subjectivity_score)
                        })
                        
                    if 'syntactic_profile' in comprehensive_report:
                        syntactic_profile = comprehensive_report['syntactic_profile']
                        metricas_quantitativas.update({
                            "complexidade_media": syntactic_profile.get('avg_dependency_complexity', syntactic_features.dependency_complexity),
                            "diversidade_pos": syntactic_profile.get('avg_pos_diversity', syntactic_features.pos_diversity),
                            "ratio_verbos_modais": syntactic_profile.get('avg_modal_ratio', syntactic_features.modal_verb_ratio)
                        })
                        
            except Exception as e:
                # Provide default metrics if calculation fails
                metricas_quantitativas = {
                    "polaridade_media": 0.0,
                    "intensidade_emocional_media": 0.0,
                    "complexidade_media": 0.0,
                    "nivel_certeza_medio": 0.0,
                    "score_formalidade_medio": 0.0
                }
        else:
            # Use basic text analysis when advanced detector not available
            metricas_quantitativas = {
                "polaridade_media": 0.0,
                "intensidade_emocional_media": 0.0,
                "complexidade_media": 0.0,
                "nivel_certeza_medio": 0.0,
                "score_formalidade_medio": 0.0
            }

        # Calculate total segments analyzed (same as in /analyze endpoint)
        total_segments_analyzed = 0
        if ADVANCED_DETECTOR_AVAILABLE and advanced_bias_detector and advanced_bias_detector.nlp:
            doc = advanced_bias_detector.nlp(content)
            total_segments_analyzed = len([sent for sent in doc.sents if len(sent.text.strip()) >= 20])
            print(f"DEBUG DETAILED: spaCy encontrou {total_segments_analyzed} segmentos v√°lidos")
        else:
            # Fallback: estimate based on punctuation
            total_segments_analyzed = len([s.strip() for s in content.split('.') if len(s.strip()) >= 20])
            print(f"DEBUG DETAILED: Fallback encontrou {total_segments_analyzed} segmentos v√°lidos")
        
        print(f"DEBUG DETAILED: Valor final total_segments_analyzed = {total_segments_analyzed}")
        print(f"DEBUG DETAILED: Bias analyses encontradas = {len(bias_analyses)}")
        
        # Create final result
        final_result = AnalysisResponse(
            article_title=request.title,
            article_url=wikipedia_result["url"],
            article_content=content[:500] + "..." if len(content) > 500 else content,
            ai_related=ai_relevance,
            bias_detected=analysis_result.bias_detected,
            overall_bias_score=analysis_result.overall_bias_score,
            bias_categories=analysis_result.bias_categories,
            detailed_analysis=analysis_result.detailed_analysis,
            reformulated_text=reformulated_text,
            total_trechos_analisados=total_segments_analyzed,
            metricas_quantitativas=metricas_quantitativas,
            analysis_summary=f"""
An√°lise conclu√≠da com sucesso!

üìä **Resumo dos Resultados:**
- Artigo relacionado √† IA: {'Sim' if ai_relevance else 'N√£o'}
- Vi√©s detectado: {'Sim' if analysis_result.bias_detected else 'N√£o'}
- Score de vi√©s: {analysis_result.overall_bias_score:.2f}/1.0
- Categorias de vi√©s: {len(analysis_result.bias_categories)}
- M√©todo de an√°lise: {analysis_method}

üîç **Processo:**
- Tempo total: {time.time() - start_total_time:.1f}s
- Etapas executadas: {len(steps)}
- Reformula√ß√£o: {'Gerada' if reformulated_text else 'N√£o necess√°ria'}
            """.strip()
        )
        
        current_step.status = "completed"
        current_step.end_time = time.time()
        current_step.metrics = {
            "resumo_gerado": True,
            "recomenda√ß√µes": len(analysis_result.bias_categories),
            "tempo_total": time.time() - start_total_time
        }
        
        return DetailedAnalysisResponse(
            success=True,
            steps=steps,
            final_result=final_result,
            total_duration=time.time() - start_total_time
        )
        
    except Exception as e:
        # Mark current step as error
        for step in steps:
            if step.status == "running":
                step.status = "error"
                step.end_time = time.time()
                break
                
        return DetailedAnalysisResponse(
            success=False,
            steps=steps,
            error_message=f"Erro interno: {str(e)}",
            total_duration=time.time() - start_total_time
        )

if __name__ == "__main__":
    # Configura√ß√£o para desenvolvimento
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    ) 